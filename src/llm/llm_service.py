class LLMService:
    def __init__(self):
        pass

    def generate_response(self, context: str, query: str):
        """
        Generate a response from the LLM based on the provided context and query.
        
        Parameters:
            context (str): The context generated from the RAG system.
            query (str): The user's question or query.

        Returns:
            str: The response generated by the LLM.
        
        Implement the logic to communicate with the LLM hosted on Azure here.
        """
        pass

    def create_context(self, texts: list):
        """
        Create a context string from the provided list of texts.

        Parameters:
            texts (list): A list of texts returned by the RAG system.

        Returns:
            str: A single string that combines the provided texts into a coherent context.

        Implement the logic to generate a context string from the texts here.
        """
        pass

    def prompt_engineering(self, context: str, query: str):
        """
        Create a prompt for the LLM using the context and query.

        Parameters:
            context (str): The context to include in the prompt.
            query (str): The user's question or query.

        Returns:
            str: The constructed prompt for the LLM.

        Implement the logic for basic prompt engineering here, combining instructions, context, and the query.
        """
        pass